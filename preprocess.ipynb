{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c571d7b-a13e-4400-b153-32ab356e8a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, json\n",
    "import pandas as pd\n",
    "\n",
    "# ✅ 디렉토리 설정\n",
    "BASE_DIR = \"data\"\n",
    "RAW_DIR = os.path.join(BASE_DIR, \"raw\")\n",
    "OUT_DIR = os.path.join(BASE_DIR, \"processed\")\n",
    "\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "INPUT_FILES = {\n",
    "    \"EU_AI_act\": os.path.join(RAW_DIR, \"EU_AI_act.txt\"),\n",
    "    \"GDPR\": os.path.join(RAW_DIR, \"GDPR.txt\"),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "639ea788-bf00-4ccd-a372-78b7af65c46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text(path):\n",
    "    for enc in (\"utf-8-sig\", \"utf-8\", \"latin-1\"):\n",
    "        try:\n",
    "            with open(path, \"r\", encoding=enc) as f:\n",
    "                return f.read()\n",
    "        except Exception:\n",
    "            continue\n",
    "    raise RuntimeError(f\"Failed to read {path} with common encodings\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "771a5c12-1ca1-41eb-984a-6c9d5fc9a59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_whitespace(s: str) -> str:\n",
    "    s = s.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n",
    "    s = re.sub(r\"[ \\t]+\", \" \", s)\n",
    "    s = re.sub(r\"\\n{3,}\", \"\\n\\n\", s)\n",
    "    return s.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd29330c-9853-4b4a-881b-0cfe62d1b7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_leading_headers(s: str, key_markers):\n",
    "    low = s.lower()\n",
    "    idx = -1\n",
    "    for marker in key_markers:\n",
    "        m = low.find(marker.lower())\n",
    "        if m != -1:\n",
    "            idx = m if idx == -1 else min(idx, m)\n",
    "    if idx > 0:\n",
    "        return s[idx:]\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6fc76f45-bcca-4ed5-8fb4-943d8eda6f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_by_articles(text: str):\n",
    "    pattern = re.compile(r\"(?m)^(Article\\s+\\d+[A-Za-z\\-]*\\.?\\s*[^\\n]*)\")\n",
    "    parts = pattern.split(text)\n",
    "    if len(parts) <= 1:\n",
    "        return []\n",
    "    preface = parts[0].strip()\n",
    "    out = []\n",
    "    for i in range(1, len(parts), 2):\n",
    "        header = parts[i].strip()\n",
    "        body = parts[i+1].strip() if i+1 < len(parts) else \"\"\n",
    "        m = re.search(r\"Article\\s+(\\d+[A-Za-z\\-]*)\", header)\n",
    "        art_num = m.group(1) if m else f\"Unknown_{i//2}\"\n",
    "        out.append((art_num, header + \"\\n\" + body))\n",
    "    if preface:\n",
    "        out.insert(0, (\"Recitals\", preface))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e01067b-985c-4963-8fc4-640c21ed5a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def paragraph_chunks(text: str, max_chars=1200):\n",
    "    paras = [p.strip() for p in text.split(\"\\n\\n\") if p.strip()]\n",
    "    merged = []\n",
    "    buff = \"\"\n",
    "    for p in paras:\n",
    "        if not buff:\n",
    "            buff = p\n",
    "        elif len(buff) + 2 + len(p) <= max_chars:\n",
    "            buff = buff + \"\\n\\n\" + p\n",
    "        else:\n",
    "            merged.append(buff)\n",
    "            buff = p\n",
    "    if buff:\n",
    "        merged.append(buff)\n",
    "    return merged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18174de7-7b69-4fec-a4f5-dbccd10f924a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_article(article_id: str, text: str, namespace: str, max_chars=1200):\n",
    "    chunks = paragraph_chunks(text, max_chars=max_chars)\n",
    "    rows = []\n",
    "    for i, ch in enumerate(chunks, start=1):\n",
    "        cid = f\"{namespace}_Article_{article_id}_p{i}\" if article_id != \"Recitals\" else f\"{namespace}_Recitals_p{i}\"\n",
    "        rows.append({\"id\": cid, \"source\": namespace, \"article\": article_id, \"text\": ch})\n",
    "    return rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1fae821-1bb4-49f4-b2a1-844d649eed1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_file(ns_name: str, raw_text: str):\n",
    "    t = normalize_whitespace(raw_text)\n",
    "    markers = [\"REGULATION (EU)\", \"Artificial Intelligence Act\", \"General Data Protection Regulation\", \"REGULATION (EU) 2016/679\", \"REGULATION (EU) 2024/1689\"]\n",
    "    t = strip_leading_headers(t, markers)\n",
    "    arts = split_by_articles(t)\n",
    "\n",
    "    rows = []\n",
    "    if arts:\n",
    "        for art_num, block in arts:\n",
    "            rows.extend(chunk_article(art_num, block, ns_name))\n",
    "    else:\n",
    "        for i, ch in enumerate(paragraph_chunks(t), start=1):\n",
    "            rows.append({\"id\": f\"{ns_name}_p{i}\", \"source\": ns_name, \"article\": \"Unknown\", \"text\": ch})\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "39fe9dad-5272-4795-870d-6a558698060c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_rows = []\n",
    "for ns, path in INPUT_FILES.items():\n",
    "    txt = read_text(path)\n",
    "    rows = preprocess_file(ns, txt)\n",
    "    all_rows.extend(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9970153a-33be-440a-81b6-3417f6b6c115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ JSONL saved to: data/processed/knowledge_base.jsonl\n",
      "✅ CSV saved to:   data/processed/knowledge_base.csv\n",
      "Total chunks: 270\n"
     ]
    }
   ],
   "source": [
    "jsonl_path = os.path.join(OUT_DIR, \"knowledge_base.jsonl\")\n",
    "csv_path = os.path.join(OUT_DIR, \"knowledge_base.csv\")\n",
    "\n",
    "with open(jsonl_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for r in all_rows:\n",
    "        f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "pd.DataFrame(all_rows).to_csv(csv_path, index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(f\"✅ JSONL saved to: {jsonl_path}\")\n",
    "print(f\"✅ CSV saved to:   {csv_path}\")\n",
    "print(f\"Total chunks: {len(all_rows)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f084ea6a-d027-4009-99c2-811528d48d67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
